"""
JobSearchAgent - èŒä½æœç´¢Agent
JobSearchAgent - Job search agent using Claude 4 and external APIs
æ ¹æ®å¼€å‘æ–‡æ¡£ç¬¬5èŠ‚è¦æ±‚å®ç°å¤šæºèŒä½æœç´¢å’Œèšåˆ
Implementing multi-source job search and aggregation as per section 5 of development documentation
"""

import logging
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone

import httpx
from langchain_core.tools import BaseTool
from langchain_core.messages import HumanMessage, AIMessage
from pydantic import BaseModel, Field

from app.agents.base import BaseAgent, AgentState
from app.services.azure_search import get_search_service, JobDocument
from app.core.config import settings


class WebSearchInput(BaseModel):
    """Webæœç´¢å·¥å…·è¾“å…¥å‚æ•°æ¨¡å‹"""
    query: str = Field(description="æœç´¢å…³é”®è¯ï¼Œå¦‚'python developer'æˆ–'data scientist'")
    location: str = Field(default="Berlin", description="å·¥ä½œåœ°ç‚¹ï¼Œå¦‚'Berlin'ã€'Munich'æˆ–'Remote'")
    limit: int = Field(default=20, description="è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤20ï¼Œæœ€å¤§50")


class LocalJobSearchInput(BaseModel):
    """æœ¬åœ°èŒä½æœç´¢å·¥å…·è¾“å…¥å‚æ•°æ¨¡å‹"""
    search_text: str = Field(description="æœç´¢æ–‡æœ¬ï¼Œæ”¯æŒèŒä½æ ‡é¢˜ã€æŠ€èƒ½ã€å…¬å¸å")
    top: int = Field(default=10, description="è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤10")


class WebSearchTool(BaseTool):
    """
    å¢å¼ºçš„ç½‘ç»œæœç´¢å·¥å…· - æ•´åˆå¤šä¸ªæ•°æ®æº
    Enhanced web search tool - integrates multiple data sources
    """
    
    name: str = "web_search_20250305"
    description: str = """
    ä½¿ç”¨å¤–éƒ¨APIæœç´¢æœ€æ–°èŒä½ä¿¡æ¯ã€‚æ•´åˆStepStoneã€Google Jobsç­‰å¤šä¸ªæ•°æ®æºã€‚
    è¿”å›JSONæ ¼å¼çš„èŒä½åˆ—è¡¨ï¼ŒåŒ…å«æ ‡é¢˜ã€å…¬å¸ã€åœ°ç‚¹ã€è–ªèµ„ç­‰ä¿¡æ¯ã€‚
    Search for latest job information using external APIs. Integrates StepStone, Google Jobs and other sources.
    Returns JSON formatted job list with title, company, location, salary and other details.
    """
    args_schema: type[WebSearchInput] = WebSearchInput
    
    def _run(self, query: str, location: str = "Berlin", limit: int = 20) -> str:
        """åŒæ­¥æœç´¢åŒ…è£…å™¨"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self._arun(query, location, limit))
    
    async def _arun(self, query: str, location: str = "Berlin", limit: int = 20) -> str:
        """
        å¼‚æ­¥æœç´¢å¤–éƒ¨API - å¹¶è¡Œè°ƒç”¨æé«˜æ•ˆç‡
        Asynchronous external API search - parallel calls for efficiency
        """
        results = []
        
        # å¹¶è¡Œè°ƒç”¨å¤šä¸ªæ•°æ®æº / Parallel calls to multiple data sources
        tasks = []
        
        # 1. StepStone via Apify
        if settings.APIFY_TOKEN:
            tasks.append(self._search_stepstone(query, location, limit // 2))
        
        # 2. Google Jobs via SerpAPI
        if settings.SERPAPI_KEY:
            tasks.append(self._search_google_jobs(query, location, limit // 2))
        
        if not tasks:
            return json.dumps({
                "status": "error",
                "message": "æœªé…ç½®å¤–éƒ¨APIå¯†é’¥ï¼Œæ— æ³•æœç´¢å¤–éƒ¨èŒä½æ•°æ®",
                "jobs": []
            }, ensure_ascii=False)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢ä»»åŠ¡ / Execute all search tasks in parallel
        search_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœ / Merge results
        for result in search_results:
            if isinstance(result, list):
                results.extend(result)
            elif isinstance(result, Exception):
                logging.error(f"æœç´¢ä»»åŠ¡å¤±è´¥: {result}")
        
        # å»é‡å’Œæ’åº / Deduplicate and sort
        unique_results = self._deduplicate_jobs(results)
        
        return json.dumps({
            "status": "success",
            "total": len(unique_results),
            "sources": ["stepstone", "google"] if len(tasks) > 1 else ["stepstone" if settings.APIFY_TOKEN else "google"],
            "jobs": unique_results[:limit]
        }, ensure_ascii=False, indent=2)
    
    async def _search_stepstone(self, query: str, location: str, limit: int) -> List[Dict[str, Any]]:
        """
        æœç´¢StepStoneèŒä½ - å¾·å›½ä¸»è¦æ±‚èŒå¹³å°
        Search StepStone jobs - major German job platform
        """
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                # æ ¹æ®å¼€å‘æ–‡æ¡£ä½¿ç”¨Apify StepStone Scraper
                url = "https://api.apify.com/v2/acts/apify~stepstone-scraper/run-sync-get-dataset-items"
                params = {"token": settings.APIFY_TOKEN}
                data = {
                    "search": query,
                    "location": location,
                    "maxItems": limit,
                    "extendOutputFunction": "",
                    "customMapFunction": ""
                }
                
                response = await client.post(url, params=params, json=data)
                response.raise_for_status()
                
                jobs_data = response.json()
                jobs = []
                
                for item in jobs_data[:limit]:
                    job = {
                        "id": f"stepstone_{item.get('id', hash(item.get('url', '')))}",
                        "title": item.get("positionName", "").strip(),
                        "company": item.get("companyName", "").strip(),
                        "location": item.get("location", location).strip(),
                        "salary": item.get("salary", "").strip(),
                        "description": self._clean_description(item.get("description", "")),
                        "url": item.get("url", ""),
                        "source": "stepstone",
                        "skills": item.get("skills", []) or self._extract_skills_from_text(item.get("description", "")),
                        "posted_date": item.get("postedTime", ""),
                        "job_type": item.get("jobType", ""),
                        "experience_level": item.get("experienceLevel", "")
                    }
                    
                    # è¿‡æ»¤æ— æ•ˆèŒä½ / Filter invalid jobs
                    if job["title"] and job["company"]:
                        jobs.append(job)
                
                logging.info(f"StepStoneæœç´¢ '{query}' åœ¨ '{location}' è¿”å› {len(jobs)} ä¸ªèŒä½")
                return jobs
                
        except Exception as e:
            logging.error(f"StepStoneæœç´¢å¤±è´¥: {e}")
            return []
    
    async def _search_google_jobs(self, query: str, location: str, limit: int) -> List[Dict[str, Any]]:
        """
        æœç´¢Google Jobs - æ•´åˆLinkedInå’ŒIndeed
        Search Google Jobs - integrates LinkedIn and Indeed
        """
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                # æ ¹æ®å¼€å‘æ–‡æ¡£ä½¿ç”¨SerpAPI Google Jobs
                url = "https://serpapi.com/search.json"
                params = {
                    "q": f"{query} jobs",
                    "location": location,
                    "engine": "google_jobs",
                    "api_key": settings.SERPAPI_KEY,
                    "num": limit,
                    "hl": "en",  # è‹±æ–‡ç»“æœ
                    "gl": "de"   # å¾·å›½åœ°åŒº
                }
                
                response = await client.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                jobs = []
                
                for item in data.get("jobs_results", [])[:limit]:
                    # æå–è–ªèµ„ä¿¡æ¯ / Extract salary information
                    salary_info = ""
                    if "salary" in item:
                        salary_info = item["salary"]
                    elif "extensions" in item:
                        salary_extensions = [ext for ext in item["extensions"] if any(char.isdigit() for char in ext)]
                        if salary_extensions:
                            salary_info = salary_extensions[0]
                    
                    job = {
                        "id": f"google_{item.get('job_id', hash(item.get('link', '')))}",
                        "title": item.get("title", "").strip(),
                        "company": item.get("company_name", "").strip(),
                        "location": item.get("location", location).strip(),
                        "salary": salary_info,
                        "description": self._clean_description(item.get("description", "")),
                        "url": item.get("link", item.get("share_link", "")),
                        "source": "google_jobs",
                        "skills": self._extract_skills_from_text(item.get("description", "")),
                        "posted_date": item.get("detected_extensions", {}).get("posted_at", ""),
                        "job_type": item.get("schedule_type", ""),
                        "via": item.get("via", "")
                    }
                    
                    # è¿‡æ»¤æ— æ•ˆèŒä½ / Filter invalid jobs
                    if job["title"] and job["company"]:
                        jobs.append(job)
                
                logging.info(f"Google Jobsæœç´¢ '{query}' åœ¨ '{location}' è¿”å› {len(jobs)} ä¸ªèŒä½")
                return jobs
                
        except Exception as e:
            logging.error(f"Google Jobsæœç´¢å¤±è´¥: {e}")
            return []
    
    def _deduplicate_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        èŒä½å»é‡ - åŸºäºæ ‡é¢˜å’Œå…¬å¸å
        Job deduplication - based on title and company
        """
        seen = set()
        unique_jobs = []
        
        for job in jobs:
            # åˆ›å»ºå»é‡é”® / Create deduplication key
            key = f"{job.get('title', '').lower()}_{job.get('company', '').lower()}"
            if key not in seen and key != "_":
                seen.add(key)
                unique_jobs.append(job)
        
        # æŒ‰ç…§æ¥æºå’Œå‘å¸ƒæ—¶é—´æ’åº / Sort by source and posting time
        return sorted(unique_jobs, key=lambda x: (x.get('source') == 'stepstone', x.get('posted_date', '')), reverse=True)
    
    def _clean_description(self, description: str, max_length: int = 500) -> str:
        """æ¸…ç†èŒä½æè¿°æ–‡æœ¬"""
        if not description:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½ / Remove HTML tags and excess whitespace
        import re
        cleaned = re.sub(r'<[^>]+>', '', description)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # é™åˆ¶é•¿åº¦ / Limit length
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned
    
    def _extract_skills_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æŠ€èƒ½å…³é”®è¯"""
        if not text:
            return []
        
        # å¸¸è§æŠ€èƒ½å…³é”®è¯ / Common skill keywords
        common_skills = [
            "python", "java", "javascript", "react", "node.js", "typescript",
            "sql", "postgresql", "mysql", "mongodb", "redis",
            "aws", "azure", "docker", "kubernetes", "terraform",
            "git", "ci/cd", "jenkins", "jira", "agile", "scrum",
            "machine learning", "data science", "ai", "tensorflow", "pytorch"
        ]
        
        text_lower = text.lower()
        found_skills = [skill for skill in common_skills if skill in text_lower]
        
        return found_skills[:10]  # é™åˆ¶æ•°é‡


class LocalJobSearchTool(BaseTool):
    """
    æœ¬åœ°èŒä½æœç´¢å·¥å…· - Azure AI Search
    Local job search tool - Azure AI Search
    """
    
    name: str = "query_local_jobs"
    description: str = """
    æœç´¢æœ¬åœ°æ•°æ®åº“ä¸­å·²ç´¢å¼•çš„èŒä½ä¿¡æ¯ã€‚ä½¿ç”¨Azure AI Searchè¿›è¡Œè¯­ä¹‰æ£€ç´¢ã€‚
    é€‚ç”¨äºæŸ¥æ‰¾å·²ç¼“å­˜çš„èŒä½æ•°æ®ï¼Œé€Ÿåº¦å¿«ä¸”æˆæœ¬ä½ã€‚
    Search indexed job information in local database using Azure AI Search semantic retrieval.
    Suitable for finding cached job data with fast speed and low cost.
    """
    args_schema: type[LocalJobSearchInput] = LocalJobSearchInput
    
    def __init__(self):
        super().__init__()
    
    def _run(self, search_text: str, top: int = 10) -> str:
        """åŒæ­¥æœç´¢åŒ…è£…å™¨"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self._arun(search_text, top))
    
    async def _arun(self, search_text: str, top: int = 10) -> str:
        """å¼‚æ­¥æœ¬åœ°æœç´¢"""
        try:
            # æ¯æ¬¡éƒ½è·å–æœç´¢æœåŠ¡ / Get search service each time
            search_service = await get_search_service()
            
            # æ‰§è¡Œè¯­ä¹‰æœç´¢ / Perform semantic search
            results = await search_service.search_jobs(
                query=search_text,
                top=top
            )
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ / Convert to unified format
            jobs = []
            for result in results:
                job = {
                    "id": result.get("id", ""),
                    "title": result.get("title", ""),
                    "company": result.get("company", ""),
                    "location": result.get("location", ""),
                    "salary": result.get("salary", ""),
                    "description": result.get("description", ""),
                    "url": result.get("url", ""),
                    "source": result.get("source", "local"),
                    "skills": result.get("skills", []),
                    "posted_date": result.get("posted_date", ""),
                    "score": result.get("@search.score", 0)
                }
                jobs.append(job)
            
            return json.dumps({
                "status": "success",
                "total": len(jobs),
                "source": "local_database",
                "jobs": jobs
            }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logging.error(f"æœ¬åœ°èŒä½æœç´¢å¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"æœ¬åœ°æœç´¢å¤±è´¥: {str(e)}",
                "jobs": []
            }, ensure_ascii=False)


class JobSearchAgent(BaseAgent):
    """
    å¢å¼ºçš„èŒä½æœç´¢Agent - å®ç°å¤šæºæ•°æ®èšåˆ
    Enhanced job search agent - implementing multi-source data aggregation
    """
    
    def __init__(self):
        super().__init__(
            name="JobSearchAgent",
            description="æ™ºèƒ½èŒä½æœç´¢ä¸“å®¶ï¼Œæ•´åˆå¤šä¸ªæ•°æ®æºæä¾›å…¨é¢çš„èŒä½ä¿¡æ¯ï¼Œæ”¯æŒå®æ—¶æœç´¢å’Œæœ¬åœ°ç¼“å­˜"
        )
        self.search_service = None
    
    def _setup_tools(self) -> None:
        """è®¾ç½®å¢å¼ºçš„å·¥å…·é›†"""
        self.tools = [
            WebSearchTool(),
            LocalJobSearchTool()
        ]
    
    def get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯JobCatcherçš„æ™ºèƒ½èŒä½æœç´¢ä¸“å®¶ã€‚ä½ çš„æ ¸å¿ƒèŒè´£ï¼š

## ğŸ¯ ä¸»è¦åŠŸèƒ½
1. **å¤šæºæ•°æ®èšåˆ**ï¼šæ•´åˆStepStoneã€Google Jobsã€LinkedInã€Indeedç­‰æ•°æ®æº
2. **æ™ºèƒ½æœç´¢ç­–ç•¥**ï¼šä¼˜åŒ–æœç´¢å…³é”®è¯ï¼Œæé«˜ç»“æœç›¸å…³æ€§
3. **æ•°æ®è´¨é‡æ§åˆ¶**ï¼šå»é‡ã€æ¸…ç†ã€éªŒè¯èŒä½ä¿¡æ¯
4. **ç¼“å­˜ç®¡ç†**ï¼šæ›´æ–°æœ¬åœ°æ•°æ®åº“ï¼Œæé«˜åç»­æœç´¢æ•ˆç‡

## ğŸ› ï¸ å·¥å…·ä½¿ç”¨ç­–ç•¥
- **web_search_20250305**: è·å–æœ€æ–°å¤–éƒ¨èŒä½æ•°æ®
  - å¹¶è¡Œè°ƒç”¨StepStoneå’ŒGoogle Jobs API
  - æ”¯æŒåœ°ç‚¹ã€å…³é”®è¯ã€æ•°é‡ç­‰å‚æ•°
  - è‡ªåŠ¨å»é‡å’Œæ•°æ®æ¸…ç†
  
- **query_local_jobs**: æœç´¢æœ¬åœ°ç¼“å­˜æ•°æ®
  - ä½¿ç”¨Azure AI Searchè¯­ä¹‰æ£€ç´¢
  - é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ï¼Œé€‚åˆé¦–æ¬¡æŸ¥è¯¢
  - æä¾›ç›¸å…³æ€§è¯„åˆ†

## ğŸ“‹ å·¥ä½œæµç¨‹
1. **é¦–å…ˆæœç´¢æœ¬åœ°**ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³ç¼“å­˜æ•°æ®
2. **è¡¥å……å¤–éƒ¨æ•°æ®**ï¼šè°ƒç”¨å¤–éƒ¨APIè·å–æœ€æ–°èŒä½
3. **æ•°æ®æ•´åˆ**ï¼šåˆå¹¶ã€å»é‡ã€æ’åºç»“æœ
4. **ç¼“å­˜æ›´æ–°**ï¼šå°†æ–°èŒä½æ•°æ®å­˜å‚¨åˆ°æœ¬åœ°æ•°æ®åº“
5. **ç»“æœä¼˜åŒ–**ï¼šæŒ‰ç›¸å…³æ€§å’Œæ–°é²œåº¦æ’åº

## ğŸ¨ è¾“å‡ºæ ¼å¼
å§‹ç»ˆè¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®ï¼ŒåŒ…å«ï¼š
- status: æœç´¢çŠ¶æ€
- total: ç»“æœæ€»æ•°
- sources: æ•°æ®æ¥æº
- jobs: èŒä½è¯¦ç»†ä¿¡æ¯æ•°ç»„

æ¯ä¸ªèŒä½åŒ…å«ï¼štitle, company, location, salary, description, url, source, skills, posted_date

## ğŸ” æœç´¢ä¼˜åŒ–
- ç†è§£ç”¨æˆ·æ„å›¾ï¼Œä¼˜åŒ–æœç´¢å…³é”®è¯
- æ”¯æŒæŠ€èƒ½ã€èŒä½çº§åˆ«ã€å·¥ä½œç±»å‹ç­‰å¤šç»´åº¦æœç´¢
- è‡ªåŠ¨å¤„ç†åœ°ç‚¹ä¿¡æ¯ï¼ˆBerlin, Munich, Remoteç­‰ï¼‰
- æ™ºèƒ½åŒ¹é…ç”¨æˆ·æŠ€èƒ½å’ŒèŒä½è¦æ±‚

å§‹ç»ˆæä¾›é«˜è´¨é‡ã€ç›¸å…³æ€§å¼ºçš„èŒä½æœç´¢ç»“æœï¼"""
    
    async def invoke(self, state: AgentState) -> Dict[str, Any]:
        """æ‰§è¡ŒèŒä½æœç´¢"""
        try:
            # è·å–æœç´¢æœåŠ¡å’Œå·¥å…·
            search_service = await get_search_service()
            local_search_tool = search_service.get_search_tool()
            
            # ä¸´æ—¶æ·»åŠ æœ¬åœ°æœç´¢å·¥å…·
            if local_search_tool not in self.tools:
                self.tools.append(local_search_tool)
                self.llm_with_tools = self.llm.bind_tools(self.tools)
            
            # æå–æœç´¢æŸ¥è¯¢
            search_query = state.get("search_query", "")
            if not search_query:
                # ä»æœ€æ–°æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢
                messages = state.get("messages", [])
                if messages:
                    last_message = messages[-1]
                    if hasattr(last_message, 'content'):
                        search_query = last_message.content
                    elif isinstance(last_message, dict):
                        search_query = last_message.get("content", "")
            
            if not search_query:
                return {
                    "messages": state["messages"] + [AIMessage(content="è¯·æä¾›æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š'Pythonå¼€å‘å·¥ç¨‹å¸ˆ'æˆ–'Frontend Developer'")],
                    "job_results": []
                }
            
            self.logger.info(f"å¼€å§‹èŒä½æœç´¢ï¼š{search_query}")
            
            # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ‰§è¡ŒClaude 4å¤„ç†
            result = await super().invoke(state)
            
            # è§£æå’Œå¤„ç†æœç´¢ç»“æœ
            try:
                response_content = result.get("last_response", "")
                
                # Claude 4åº”è¯¥è¿”å›ç»“æ„åŒ–çš„èŒä½æ•°æ®
                # è¿™é‡Œæ·»åŠ è§£æé€»è¾‘ï¼Œæå–JSONæ ¼å¼çš„èŒä½åˆ—è¡¨
                jobs = self._extract_jobs_from_response(response_content)
                
                # ç¼“å­˜æ–°èŒä½åˆ°æ•°æ®åº“
                await self._cache_jobs_to_database(jobs, search_service)
                
                result["job_results"] = jobs
                result["search_query"] = search_query
                
                self.logger.info(f"èŒä½æœç´¢å®Œæˆï¼Œè¿”å› {len(jobs)} ä¸ªèŒä½")
                
            except Exception as e:
                self.logger.error(f"å¤„ç†æœç´¢ç»“æœå¤±è´¥: {e}")
                result["job_results"] = []
            
            return result
            
        except Exception as e:
            self.logger.error(f"JobSearchAgentæ‰§è¡Œå¤±è´¥: {e}")
            return {
                "messages": state["messages"] + [AIMessage(content=f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")],
                "job_results": [],
                "error": str(e)
            }
    
    def _extract_jobs_from_response(self, response: str) -> List[Dict[str, Any]]:
        """ä»Claude 4å“åº”ä¸­æå–èŒä½æ•°æ®"""
        try:
            # æŸ¥æ‰¾JSONæ ¼å¼çš„èŒä½æ•°æ®
            import re
            
            # å°è¯•æå–JSONæ•°ç»„
            json_pattern = r'\[[\s\S]*?\]'
            matches = re.findall(json_pattern, response)
            
            for match in matches:
                try:
                    jobs = json.loads(match)
                    if isinstance(jobs, list) and jobs:
                        return jobs
                except json.JSONDecodeError:
                    continue
            
            # å¦‚æœæ‰¾ä¸åˆ°JSONï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
            
        except Exception as e:
            self.logger.error(f"æå–èŒä½æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def _cache_jobs_to_database(self, jobs: List[Dict[str, Any]], search_service) -> None:
        """å°†æ–°èŒä½ç¼“å­˜åˆ°æ•°æ®åº“"""
        try:
            job_documents = []
            
            for job in jobs:
                if not job.get("id") or not job.get("title"):
                    continue
                
                job_doc = JobDocument(
                    id=job["id"],
                    title=job["title"],
                    company=job.get("company", ""),
                    location=job.get("location", ""),
                    salary=job.get("salary"),
                    description=job.get("description", ""),
                    skills=job.get("skills", []),
                    source=job.get("source", "unknown"),
                    url=job.get("url", ""),
                    indexed_at=datetime.now(timezone.utc),
                    expired=False
                )
                job_documents.append(job_doc)
            
            if job_documents:
                success_count = await search_service.index_jobs_batch(job_documents)
                self.logger.info(f"æˆåŠŸç¼“å­˜ {success_count}/{len(job_documents)} ä¸ªèŒä½åˆ°æ•°æ®åº“")
            
        except Exception as e:
            self.logger.error(f"ç¼“å­˜èŒä½åˆ°æ•°æ®åº“å¤±è´¥: {e}")


# Agentå®ä¾‹åŒ–
# Agent instantiation
job_search_agent = JobSearchAgent() 