"""
èŒä½æœç´¢Agent
Job Search Agent for aggregating job data from multiple sources
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

from langchain_core.tools import tool
from pydantic import BaseModel, Field

from app.agents.base import BaseAgent, AgentState
from app.services.job_sources import StepStoneService, GoogleJobsService
from app.services.azure_search import AzureSearchService


class JobSearchCriteria(BaseModel):
    """
    èŒä½æœç´¢æ¡ä»¶æ¨¡å‹
    Job search criteria model
    """
    keywords: str = Field(description="æœç´¢å…³é”®è¯ / Search keywords")
    location: Optional[str] = Field(default=None, description="å·¥ä½œåœ°ç‚¹ / Job location")
    job_type: Optional[str] = Field(default=None, description="å·¥ä½œç±»å‹ / Job type")
    salary_min: Optional[int] = Field(default=None, description="æœ€ä½è–ªèµ„ / Minimum salary")
    salary_max: Optional[int] = Field(default=None, description="æœ€é«˜è–ªèµ„ / Maximum salary")
    experience_level: Optional[str] = Field(default=None, description="ç»éªŒæ°´å¹³ / Experience level")
    remote_allowed: bool = Field(default=False, description="æ˜¯å¦å…è®¸è¿œç¨‹ / Remote work allowed")
    max_results: int = Field(default=20, description="æœ€å¤§ç»“æœæ•° / Maximum results")


class JobSearchAgent(BaseAgent):
    """
    èŒä½æœç´¢Agent
    Responsible for searching and aggregating job postings from multiple sources
    """
    
    def __init__(self):
        super().__init__(
            name="job_search_agent",
            description="ä¸“ä¸šçš„èŒä½æœç´¢ä¸“å®¶ï¼Œèƒ½å¤Ÿä»å¤šä¸ªæ•°æ®æºèšåˆèŒä½ä¿¡æ¯ / Professional job search expert that aggregates job information from multiple sources",
            temperature=0.1
        )
        
        # åˆå§‹åŒ–æœåŠ¡
        # Initialize services
        self.stepstone_service = StepStoneService()
        self.google_jobs_service = GoogleJobsService()
        self.azure_search_service = AzureSearchService()
        
        self.logger = logging.getLogger("agent.job_search")
    
    def _setup_tools(self) -> None:
        """
        è®¾ç½®èŒä½æœç´¢ç›¸å…³å·¥å…·
        Setup job search related tools
        """
        
        @tool("search_jobs_stepstone")
        def search_stepstone(
            keywords: str,
            location: str = None,
            max_results: int = 10
        ) -> Dict[str, Any]:
            """
            ä»StepStoneæœç´¢èŒä½
            Search jobs from StepStone platform
            """
            try:
                results = asyncio.run(
                    self.stepstone_service.search_jobs(
                        keywords=keywords,
                        location=location,
                        max_results=max_results
                    )
                )
                return {
                    "success": True,
                    "source": "stepstone",
                    "count": len(results),
                    "jobs": results
                }
            except Exception as e:
                self.logger.error(f"StepStoneæœç´¢å¤±è´¥ / StepStone search failed: {e}")
                return {
                    "success": False,
                    "source": "stepstone",
                    "error": str(e),
                    "jobs": []
                }
        
        @tool("search_jobs_google")
        def search_google_jobs(
            keywords: str,
            location: str = None,
            max_results: int = 10
        ) -> Dict[str, Any]:
            """
            ä»Google Jobsæœç´¢èŒä½
            Search jobs from Google Jobs
            """
            try:
                results = asyncio.run(
                    self.google_jobs_service.search_jobs(
                        keywords=keywords,
                        location=location,
                        max_results=max_results
                    )
                )
                return {
                    "success": True,
                    "source": "google_jobs",
                    "count": len(results),
                    "jobs": results
                }
            except Exception as e:
                self.logger.error(f"Google Jobsæœç´¢å¤±è´¥ / Google Jobs search failed: {e}")
                return {
                    "success": False,
                    "source": "google_jobs", 
                    "error": str(e),
                    "jobs": []
                }
        
        @tool("search_jobs_vector")
        def search_jobs_semantic(
            query: str,
            filters: Dict[str, Any] = None,
            max_results: int = 10
        ) -> Dict[str, Any]:
            """
            ä½¿ç”¨å‘é‡è¯­ä¹‰æœç´¢èŒä½
            Search jobs using vector semantic search
            """
            try:
                results = asyncio.run(
                    self.azure_search_service.semantic_search(
                        query=query,
                        filters=filters,
                        top_k=max_results
                    )
                )
                return {
                    "success": True,
                    "source": "vector_search",
                    "count": len(results),
                    "jobs": results
                }
            except Exception as e:
                self.logger.error(f"å‘é‡æœç´¢å¤±è´¥ / Vector search failed: {e}")
                return {
                    "success": False,
                    "source": "vector_search",
                    "error": str(e),
                    "jobs": []
                }
        
        @tool("aggregate_job_results")
        def aggregate_and_deduplicate(
            job_lists: List[Dict[str, Any]],
            max_final_results: int = 20
        ) -> Dict[str, Any]:
            """
            èšåˆå’Œå»é‡èŒä½ç»“æœ
            Aggregate and deduplicate job results
            """
            try:
                all_jobs = []
                source_stats = {}
                
                # åˆå¹¶æ‰€æœ‰æ¥æºçš„èŒä½
                # Merge jobs from all sources
                for job_list in job_lists:
                    if job_list.get("success") and job_list.get("jobs"):
                        source = job_list.get("source", "unknown")
                        jobs = job_list["jobs"]
                        
                        source_stats[source] = len(jobs)
                        all_jobs.extend(jobs)
                
                # å»é‡ (åŸºäºèŒä½æ ‡é¢˜å’Œå…¬å¸åç§°)
                # Deduplicate based on job title and company
                seen = set()
                unique_jobs = []
                
                for job in all_jobs:
                    key = f"{job.get('title', '').lower()}_{job.get('company', '').lower()}"
                    if key not in seen:
                        seen.add(key)
                        unique_jobs.append(job)
                
                # æŒ‰ç›¸å…³æ€§å’Œæ—¶é—´æ’åº
                # Sort by relevance and time
                unique_jobs.sort(
                    key=lambda x: (
                        x.get('relevance_score', 0),
                        x.get('posted_at', datetime.min)
                    ),
                    reverse=True
                )
                
                # é™åˆ¶ç»“æœæ•°é‡
                # Limit results
                final_jobs = unique_jobs[:max_final_results]
                
                return {
                    "success": True,
                    "total_found": len(all_jobs),
                    "unique_count": len(unique_jobs),
                    "final_count": len(final_jobs),
                    "source_stats": source_stats,
                    "jobs": final_jobs
                }
                
            except Exception as e:
                self.logger.error(f"èšåˆå¤±è´¥ / Aggregation failed: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "jobs": []
                }
        
        # æ·»åŠ ç§»äº¤å·¥å…·
        # Add handoff tools
        transfer_to_resume_critic = self.create_handoff_tool(
            target_agent="resume_critic_agent",
            description="å°†èŒä½ç»“æœç§»äº¤ç»™ç®€å†åˆ†æä¸“å®¶è¿›è¡ŒåŒ¹é…åº¦åˆ†æ / Transfer job results to resume analysis expert for match scoring"
        )
        
        transfer_to_skill_heatmap = self.create_handoff_tool(
            target_agent="skill_heatmap_agent", 
            description="å°†èŒä½æ•°æ®ç§»äº¤ç»™æŠ€èƒ½åˆ†æä¸“å®¶ç”Ÿæˆçƒ­ç‚¹å›¾ / Transfer job data to skill analysis expert for heatmap generation"
        )
        
        # æ³¨å†Œæ‰€æœ‰å·¥å…·
        # Register all tools
        self.tools = [
            search_stepstone,
            search_google_jobs,
            search_jobs_semantic,
            aggregate_and_deduplicate,
            transfer_to_resume_critic,
            transfer_to_skill_heatmap
        ]
    
    def get_system_prompt(self) -> str:
        """
        è·å–èŒä½æœç´¢Agentçš„ç³»ç»Ÿæç¤ºè¯
        Get system prompt for job search agent
        """
        return """ä½ æ˜¯JobCatcherå¹³å°çš„èŒä½æœç´¢ä¸“å®¶ã€‚ä½ çš„ä¸»è¦èŒè´£æ˜¯ï¼š

ğŸ¯ **æ ¸å¿ƒèƒ½åŠ› / Core Capabilities:**
- ä»å¤šä¸ªæ•°æ®æºï¼ˆStepStoneã€Google Jobsã€Azureæœç´¢ï¼‰èšåˆèŒä½ä¿¡æ¯
- æ™ºèƒ½è§£æç”¨æˆ·çš„æœç´¢éœ€æ±‚å’Œåå¥½
- æä¾›é«˜è´¨é‡çš„èŒä½æ¨èå’ŒåŒ¹é…
- å»é‡å’Œæ’åºèŒä½ç»“æœ

ğŸ“‹ **å·¥ä½œæµç¨‹ / Workflow:**
1. ç†è§£ç”¨æˆ·çš„èŒä½æœç´¢éœ€æ±‚ï¼ˆå…³é”®è¯ã€åœ°ç‚¹ã€è–ªèµ„ç­‰ï¼‰
2. ä½¿ç”¨å¤šä¸ªå·¥å…·å¹¶è¡Œæœç´¢ä¸åŒæ•°æ®æº
3. èšåˆå’Œå»é‡æœç´¢ç»“æœ
4. æŒ‰ç›¸å…³æ€§å¯¹èŒä½è¿›è¡Œæ’åº
5. å¦‚éœ€è¦ï¼Œç§»äº¤ç»™å…¶ä»–ä¸“å®¶Agentè¿›è¡Œè¿›ä¸€æ­¥åˆ†æ

ğŸ’¡ **æœ€ä½³å®è·µ / Best Practices:**
- ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„å…·ä½“æœç´¢æ¡ä»¶
- å¦‚æœæœç´¢ç»“æœè¿‡å°‘ï¼Œå°è¯•æ‰©å±•æœç´¢èŒƒå›´
- ä¸ºç”¨æˆ·æä¾›å¤šæ ·åŒ–çš„èŒä½é€‰æ‹©
- å®æ—¶ç›‘æ§æœç´¢è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦

ğŸ”§ **å¯ç”¨å·¥å…· / Available Tools:**
- search_jobs_stepstone: StepStoneèŒä½æœç´¢
- search_jobs_google: Google Jobsæœç´¢  
- search_jobs_vector: å‘é‡è¯­ä¹‰æœç´¢
- aggregate_job_results: èšåˆå»é‡ç»“æœ
- transfer_to_resume_critic: ç§»äº¤ç®€å†åˆ†æ
- transfer_to_skill_heatmap: ç§»äº¤æŠ€èƒ½åˆ†æ

å§‹ç»ˆä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒï¼Œæä¾›æœ€ç›¸å…³å’Œæœ‰ä»·å€¼çš„èŒä½æ¨èã€‚
Always be user-centric and provide the most relevant and valuable job recommendations."""
    
    async def search_jobs_comprehensive(
        self,
        criteria: JobSearchCriteria,
        enable_parallel: bool = True
    ) -> Dict[str, Any]:
        """
        å…¨é¢çš„èŒä½æœç´¢
        Comprehensive job search across multiple sources
        """
        try:
            self.logger.info(f"å¼€å§‹æœç´¢èŒä½ / Starting job search: {criteria.keywords}")
            
            search_tasks = []
            
            if enable_parallel:
                # å¹¶è¡Œæœç´¢æ‰€æœ‰æ•°æ®æº
                # Parallel search across all sources
                search_tasks = [
                    self.stepstone_service.search_jobs(
                        keywords=criteria.keywords,
                        location=criteria.location,
                        max_results=criteria.max_results // 3
                    ),
                    self.google_jobs_service.search_jobs(
                        keywords=criteria.keywords,
                        location=criteria.location,
                        max_results=criteria.max_results // 3
                    ),
                    self.azure_search_service.semantic_search(
                        query=f"{criteria.keywords} {criteria.location or ''}",
                        top_k=criteria.max_results // 3
                    )
                ]
                
                results = await asyncio.gather(*search_tasks, return_exceptions=True)
            else:
                # é¡ºåºæœç´¢
                # Sequential search
                results = []
                
                for service, method in [
                    (self.stepstone_service, "search_jobs"),
                    (self.google_jobs_service, "search_jobs"),
                    (self.azure_search_service, "semantic_search")
                ]:
                    try:
                        if method == "semantic_search":
                            result = await service.semantic_search(
                                query=f"{criteria.keywords} {criteria.location or ''}",
                                top_k=criteria.max_results // 3
                            )
                        else:
                            result = await service.search_jobs(
                                keywords=criteria.keywords,
                                location=criteria.location,
                                max_results=criteria.max_results // 3
                            )
                        results.append(result)
                    except Exception as e:
                        self.logger.warning(f"æœç´¢æºå¤±è´¥ / Search source failed: {e}")
                        results.append([])
            
            # èšåˆç»“æœ
            # Aggregate results
            all_jobs = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.warning(f"æœç´¢ä»»åŠ¡ {i} å¤±è´¥ / Search task {i} failed: {result}")
                    continue
                
                if isinstance(result, list):
                    all_jobs.extend(result)
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            # Apply filters
            filtered_jobs = self._apply_filters(all_jobs, criteria)
            
            # å»é‡å’Œæ’åº
            # Deduplicate and sort
            final_jobs = self._deduplicate_and_sort(filtered_jobs, criteria.max_results)
            
            return {
                "success": True,
                "criteria": criteria.dict(),
                "total_found": len(all_jobs),
                "filtered_count": len(filtered_jobs),
                "final_count": len(final_jobs),
                "jobs": final_jobs,
                "search_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆæœç´¢å¤±è´¥ / Comprehensive search failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "jobs": []
            }
    
    def _apply_filters(self, jobs: List[Dict], criteria: JobSearchCriteria) -> List[Dict]:
        """
        åº”ç”¨æœç´¢è¿‡æ»¤æ¡ä»¶
        Apply search filters
        """
        filtered = jobs
        
        # è–ªèµ„è¿‡æ»¤
        # Salary filtering
        if criteria.salary_min:
            filtered = [
                job for job in filtered
                if job.get('salary_min', 0) >= criteria.salary_min
            ]
        
        if criteria.salary_max:
            filtered = [
                job for job in filtered
                if job.get('salary_max', float('inf')) <= criteria.salary_max
            ]
        
        # è¿œç¨‹å·¥ä½œè¿‡æ»¤
        # Remote work filtering
        if criteria.remote_allowed:
            filtered = [
                job for job in filtered
                if job.get('is_remote') or 'remote' in job.get('location', '').lower()
            ]
        
        return filtered
    
    def _deduplicate_and_sort(self, jobs: List[Dict], max_results: int) -> List[Dict]:
        """
        å»é‡å’Œæ’åºèŒä½
        Deduplicate and sort jobs
        """
        # å»é‡
        # Deduplicate
        seen = set()
        unique_jobs = []
        
        for job in jobs:
            key = f"{job.get('title', '').lower()}_{job.get('company', '').lower()}"
            if key not in seen:
                seen.add(key)
                unique_jobs.append(job)
        
        # æ’åºï¼ˆæŒ‰ç›¸å…³æ€§ã€è–ªèµ„ã€å‘å¸ƒæ—¶é—´ï¼‰
        # Sort by relevance, salary, posting time
        unique_jobs.sort(
            key=lambda x: (
                x.get('relevance_score', 0),
                x.get('salary_max', 0),
                x.get('posted_at', datetime.min)
            ),
            reverse=True
        )
        
        return unique_jobs[:max_results] 